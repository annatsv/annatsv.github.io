<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Anna Tsvetkov // annatsv.github.io</title>
  <meta name="keywords" content="Anna Tsvetkov, Anna Tsvetlkov Brown, Anna Tsvetkov Princeton, Anna Tsvetkov Philosophy">
  <meta name="description" content="Anna Tsvetkov, Personal Website">
  <meta name="author" content="Anna Tsvetkov">

  <!-- Bootstrap Core CSS -->
  <link href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" rel="stylesheet">

  <!-- Custom CSS -->
  <link href="css/custom.css" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">

  <!-- jQuery & Bootstrap JS (only once) -->
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>
</head>

<body id="page-top" class="index">
  <!-- Portfolio -->
  <section id="portfolio">
    <div class="container">

      <!-- Header -->
      <div class="row margin-top-xl">
        <div class="col-sm-2">
          <img class="dp" src="img/social/abhraneel.png">
        </div>
        <div class="col-sm-5">
<h2 class="title">
  Anna<br>
  Tsvetkov
</h2>
        </div>
      </div>

      <!-- Bio -->
      <div class="row">
        <div class="col-sm-7">
          <div class="info">
            <div>
              <p class="margin-top-xs intro">
                I'm a Philosophy PhD and Computer Science ScM student at
                <a href="https://philosophy.brown.edu/people" target="_blank"><strong>Brown</strong></a>. I work with
                <a href="https://sites.google.com/view/adampautz/home" target="_blank"><strong>Adam Pautz<strong></a>,
                <a href="https://cs.brown.edu/people/epavlick/" target="_blank"><strong>Ellie Pavlick<strong></a>,
                <a href="https://vivo.brown.edu/display/cshill#" target="_blank"><strong>Chris Hill<strong></a>,
                and <a href="https://consc.net/" target="_blank"><strong>David Chalmers<strong></a>.
              </p>
              <p class="margin-top-xs intro">
                I'm also an AI researcher at
                <a href="https://futuretech.mit.edu/" target="_blank"><strong>MIT FutureTech</strong></a>. In the Fall, I will be an AI Postdoctoral Research Fellow at
                <a href="https://ai.princeton.edu/ai-lab" target="_blank"><strong>Princeton</strong></a>.
              </p>
              <p class="margin-top-xs intro">
                My research focuses on human-centered AI. The goal of my work is to use philosophy to guide experiments on AI that deepen our understanding of the mind and, in turn, to use insights about the mind to build explainable and ethical AI.
              </p>
              <p class="margin-top-xs intro">
                You can email me at anna_tsvetkov [at] brown.edu.
              </p>
            </div>
            <p class="intro">
              <a class="cv" href="about/abhraneel_cv.pdf" target="_blank">
                CV <i class="fa fa-long-arrow-right" aria-hidden="true"></i>
              </a>
            </p>
          </div>
        </div>
      </div>

      <!-- Publications -->
      <div class="row">
        <div class="col-sm-7 margin-top-md">
          <div class="projects">
            <h2 class="section-title">Publications</h2>
          </div>
          <p class="margin-top-sm">
            <span class="paper-title">
                Can We Interpret Artificial Neural Networks As Having Beliefs and Desires?
              </a>
            </span><br>
            <span class="authors">
              <span class="self">Anna Tsvetkov</span><br>
              <span class="venue">
                <a href="pubs/CHI2025-multiple_forecasts.pdf" target="_blank">
                  <i class="fa fa-file-o fa-lg tiny social-icons email-icon"
                     style="color:#FD926F;margin-right:6px;vertical-align:middle"></i>
                  PDF
                </a>
                &nbsp;|&nbsp;
                <a
                  data-toggle="collapse"
                  href="#abstract1"
                  aria-expanded="false"
                  aria-controls="abstract1"
                  style="cursor:pointer;"
                >
                  Abstract
                </a>
              </span>
            </span>

            <div class="collapse" id="abstract1" style="margin-top:10px;">
              <p style="font-size:12px;line-height:1.4;margin:0;">
                Can we interpret the internal workings of artificial neural networks in terms of beliefs and desires? A central aim in mechanistic interpretability is to explain the inner workings of artificial neural networks in terms that we can understand. Since we explain human beings in terms of beliefs and desires, it is natural to ask whether we can explain artificial neural networks in these terms too. In recent work, David Chalmers (2025) proposes propositional interpretability as an important approach within mechanistic interpretability, arguing that the computational states of artificial neural networks can be explained in terms of propositional attitudesâ€”states like beliefs, desires, and subjective probabilities to propositions. This paper examines the prospects for propositional interpretability as a framework for explaining the internal workings of artificial neural networks. I draw attention to a number of empirical and methodological problems with propositional interpretability that are based on a philosophical gloss of recent findings in the mechanistic interpretability literature. Some of these challenges, such as determining whether artificial neural networks encode propositions rather than unbound concepts, might be mitigated through new interpretability techniques and open up exciting avenues for future research. Other problems, including reliably mapping computational states to propositional attitudes and managing an explosion of potential propositional interpretations, present serious difficulties for the approach. The challenges should be of interest both to philosophers seeking to engage with empirical work in mechanistic interpretability and to neural network researchers aiming to develop novel interpretability methods informed by philosophical insights.
              </p>
            </div>
          </p>
        </div>
      </div>

      <!-- Contact -->
      <div class="row">
        <div class="col-sm-7 margin-top-lg">
          <div class="projects">
            <h2 class="section-title">Contact</h2>
          </div>
          <div class="margin-top-xs margin-btm-lg">
            <span>
              <i class="fa fa-envelope-o fa-lg tiny social-icons email-icon"></i>
              <a class="contact-text">anna_tsvetkov [at] brown.edu</a>
            </span> <br>
            <span>
              <i class="fa fa-github-alt fa-lg tiny social-icons email-icon"></i>
              <a href="https://www.github.com/annatsv" class="contact-text">github</a>
            </span> <br>
            <span>
              <a href="https://bsky.app/profile/annatsv.bsky.social" class="contact-text">
                <i class="fa fa-globe fa-lg tiny social-icons email-icon" style="color:#FD926F;"></i>
                bluesky
              </a>
            </span>
          </div>
        </div>
      </div>

    </div>
  </section>
</body>
</html>
